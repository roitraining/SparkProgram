{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Spark context to start a session and connect to the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/student/ROI/SparkProgram')\n",
    "from initspark import *\n",
    "sc, spark, conf = initspark()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read a text file from the local file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124796\n",
      "['The Project Gutenberg EBook of The Complete Works of William Shakespeare, by ', 'William Shakespeare', '', 'This eBook is for the use of anyone anywhere at no cost and with', 'almost no restrictions whatsoever.  You may copy it, give it away or', 're-use it under the terms of the Project Gutenberg License included', 'with this eBook or online at www.gutenberg.org', '', '** This is a COPYRIGHTED Project Gutenberg eBook, Details Below **', '**     Please follow the copyright guidelines in this file.     **']\n"
     ]
    }
   ],
   "source": [
    "shake = sc.textFile('/home/student/ROI/SparkProgram/datasets/text/shakespeare.txt')\n",
    "print(shake.count())\n",
    "print(shake.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the map method to apply a function call on each element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THE PROJECT GUTENBERG EBOOK OF THE COMPLETE WORKS OF WILLIAM SHAKESPEARE, BY ',\n",
       " 'WILLIAM SHAKESPEARE',\n",
       " '',\n",
       " 'THIS EBOOK IS FOR THE USE OF ANYONE ANYWHERE AT NO COST AND WITH',\n",
       " 'ALMOST NO RESTRICTIONS WHATSOEVER.  YOU MAY COPY IT, GIVE IT AWAY OR',\n",
       " 'RE-USE IT UNDER THE TERMS OF THE PROJECT GUTENBERG LICENSE INCLUDED',\n",
       " 'WITH THIS EBOOK OR ONLINE AT WWW.GUTENBERG.ORG',\n",
       " '',\n",
       " '** THIS IS A COPYRIGHTED PROJECT GUTENBERG EBOOK, DETAILS BELOW **',\n",
       " '**     PLEASE FOLLOW THE COPYRIGHT GUIDELINES IN THIS FILE.     **']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shake2 = shake.map(str.upper)\n",
    "shake2.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the split method you get a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The',\n",
       "  'Project',\n",
       "  'Gutenberg',\n",
       "  'EBook',\n",
       "  'of',\n",
       "  'The',\n",
       "  'Complete',\n",
       "  'Works',\n",
       "  'of',\n",
       "  'William',\n",
       "  'Shakespeare,',\n",
       "  'by',\n",
       "  ''],\n",
       " ['William', 'Shakespeare'],\n",
       " [''],\n",
       " ['This',\n",
       "  'eBook',\n",
       "  'is',\n",
       "  'for',\n",
       "  'the',\n",
       "  'use',\n",
       "  'of',\n",
       "  'anyone',\n",
       "  'anywhere',\n",
       "  'at',\n",
       "  'no',\n",
       "  'cost',\n",
       "  'and',\n",
       "  'with'],\n",
       " ['almost',\n",
       "  'no',\n",
       "  'restrictions',\n",
       "  'whatsoever.',\n",
       "  '',\n",
       "  'You',\n",
       "  'may',\n",
       "  'copy',\n",
       "  'it,',\n",
       "  'give',\n",
       "  'it',\n",
       "  'away',\n",
       "  'or'],\n",
       " ['re-use',\n",
       "  'it',\n",
       "  'under',\n",
       "  'the',\n",
       "  'terms',\n",
       "  'of',\n",
       "  'the',\n",
       "  'Project',\n",
       "  'Gutenberg',\n",
       "  'License',\n",
       "  'included'],\n",
       " ['with', 'this', 'eBook', 'or', 'online', 'at', 'www.gutenberg.org'],\n",
       " [''],\n",
       " ['**',\n",
       "  'This',\n",
       "  'is',\n",
       "  'a',\n",
       "  'COPYRIGHTED',\n",
       "  'Project',\n",
       "  'Gutenberg',\n",
       "  'eBook,',\n",
       "  'Details',\n",
       "  'Below',\n",
       "  '**'],\n",
       " ['**',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'Please',\n",
       "  'follow',\n",
       "  'the',\n",
       "  'copyright',\n",
       "  'guidelines',\n",
       "  'in',\n",
       "  'this',\n",
       "  'file.',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '**']]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shake3 = shake.map(lambda x : x.split(' '))\n",
    "shake3.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The flatMap method flattens the inner list to return one big list of strings instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'EBook',\n",
       " 'of',\n",
       " 'The',\n",
       " 'Complete',\n",
       " 'Works',\n",
       " 'of',\n",
       " 'William',\n",
       " 'Shakespeare,',\n",
       " 'by',\n",
       " '',\n",
       " 'William',\n",
       " 'Shakespeare',\n",
       " '',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shake4 = shake.flatMap(lambda x : x.split(' '))\n",
    "shake4.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelize will load manually created data into the spark cluster into an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "r = sc.parallelize(range(1,11))\n",
    "print(r.collect())\n",
    "print(r.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a folder stored on HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,Beverages,Soft drinks coffees teas beers and ales',\n",
       " '2,Condiments,Sweet and savory sauces relishes spreads and seasonings',\n",
       " '3,Confections,Desserts candies and sweet breads',\n",
       " '4,Dairy Products,Cheeses',\n",
       " '5,Grains/Cereals,Breads crackers pasta and cereal',\n",
       " '6,Meat/Poultry,Prepared meats',\n",
       " '7,Produce,Dried fruit and bean curd',\n",
       " '8,Seafood,Seaweed and fish']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('hdfs://localhost:9000/categories').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the helper function to point to the HDFS URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1,Beverages,Soft drinks coffees teas beers and ales', '2,Condiments,Sweet and savory sauces relishes spreads and seasonings', '3,Confections,Desserts candies and sweet breads', '4,Dairy Products,Cheeses', '5,Grains/Cereals,Breads crackers pasta and cereal']\n",
      "['8,Seafood,Seaweed and fish', '7,Produce,Dried fruit and bean curd', '6,Meat/Poultry,Prepared meats', '5,Grains/Cereals,Breads crackers pasta and cereal', '4,Dairy Products,Cheeses']\n",
      "['7,Produce,Dried fruit and bean curd', '4,Dairy Products,Cheeses', '8,Seafood,Seaweed and fish', '1,Beverages,Soft drinks coffees teas beers and ales', '3,Confections,Desserts candies and sweet breads']\n"
     ]
    }
   ],
   "source": [
    "cat = sc.textFile(hdfsPath('categories'))\n",
    "print(cat.takeOrdered(5))\n",
    "print(cat.top(5))\n",
    "print(cat.takeSample(False,5))\n",
    "cat.foreach(lambda x : print(x.upper)) # does not display properly in notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results in an RDD to disk. Note how it makes a folder and filles it with as many files as there are nodes solving the problem. Also you must make sure that the folder does not exist or it throws an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r /home/student/file1.txt\n",
    "cat.saveAsTextFile('/home/student/file1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1,BEVERAGES,SOFT DRINKS COFFEES TEAS BEERS AND ALES', '2,CONDIMENTS,SWEET AND SAVORY SAUCES RELISHES SPREADS AND SEASONINGS', '3,CONFECTIONS,DESSERTS CANDIES AND SWEET BREADS', '4,DAIRY PRODUCTS,CHEESES', '5,GRAINS/CEREALS,BREADS CRACKERS PASTA AND CEREAL', '6,MEAT/POULTRY,PREPARED MEATS', '7,PRODUCE,DRIED FRUIT AND BEAN CURD', '8,SEAFOOD,SEAWEED AND FISH']\n"
     ]
    }
   ],
   "source": [
    "print(cat.map(str.upper).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the string into a tuple to resemble a record structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Beverages', 'Soft drinks coffees teas beers and ales'),\n",
       " (2, 'Condiments', 'Sweet and savory sauces relishes spreads and seasonings'),\n",
       " (3, 'Confections', 'Desserts candies and sweet breads'),\n",
       " (4, 'Dairy Products', 'Cheeses'),\n",
       " (5, 'Grains/Cereals', 'Breads crackers pasta and cereal'),\n",
       " (6, 'Meat/Poultry', 'Prepared meats'),\n",
       " (7, 'Produce', 'Dried fruit and bean curd'),\n",
       " (8, 'Seafood', 'Seaweed and fish')]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat1 = cat.map(lambda x : tuple(x.split(',')))\n",
    "cat1 = cat1.map(lambda x : (int(x[0]), x[1], x[2]))\n",
    "cat1.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAB: Put the regions folder found in /home/student/ROI/datasets/northwind/csv/regions into HDFS. Read it into an RDD and convert it into a tuple shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the tuple into a dictionary as an alternative form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'CategoryID': 1,\n",
       "  'Name': 'Beverages',\n",
       "  'Description': 'Soft drinks coffees teas beers and ales'},\n",
       " {'CategoryID': 2,\n",
       "  'Name': 'Condiments',\n",
       "  'Description': 'Sweet and savory sauces relishes spreads and seasonings'},\n",
       " {'CategoryID': 3,\n",
       "  'Name': 'Confections',\n",
       "  'Description': 'Desserts candies and sweet breads'},\n",
       " {'CategoryID': 4, 'Name': 'Dairy Products', 'Description': 'Cheeses'},\n",
       " {'CategoryID': 5,\n",
       "  'Name': 'Grains/Cereals',\n",
       "  'Description': 'Breads crackers pasta and cereal'},\n",
       " {'CategoryID': 6, 'Name': 'Meat/Poultry', 'Description': 'Prepared meats'},\n",
       " {'CategoryID': 7,\n",
       "  'Name': 'Produce',\n",
       "  'Description': 'Dried fruit and bean curd'},\n",
       " {'CategoryID': 8, 'Name': 'Seafood', 'Description': 'Seaweed and fish'}]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat2 = cat1.map(lambda x : dict(zip(['CategoryID', 'Name', 'Description'], x)))\n",
    "cat2.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can chain multiple transformations together to do it all in one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'CategoryID': 1,\n",
       "  'Name': 'Beverages',\n",
       "  'Description': 'Soft drinks coffees teas beers and ales'},\n",
       " {'CategoryID': 2,\n",
       "  'Name': 'Condiments',\n",
       "  'Description': 'Sweet and savory sauces relishes spreads and seasonings'},\n",
       " {'CategoryID': 3,\n",
       "  'Name': 'Confections',\n",
       "  'Description': 'Desserts candies and sweet breads'},\n",
       " {'CategoryID': 4, 'Name': 'Dairy Products', 'Description': 'Cheeses'},\n",
       " {'CategoryID': 5,\n",
       "  'Name': 'Grains/Cereals',\n",
       "  'Description': 'Breads crackers pasta and cereal'},\n",
       " {'CategoryID': 6, 'Name': 'Meat/Poultry', 'Description': 'Prepared meats'},\n",
       " {'CategoryID': 7,\n",
       "  'Name': 'Produce',\n",
       "  'Description': 'Dried fruit and bean curd'},\n",
       " {'CategoryID': 8, 'Name': 'Seafood', 'Description': 'Seaweed and fish'}]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat2 = cat.map(lambda x : tuple(x.split(','))) \\\n",
    "      .map(lambda x : (int(x[0]), x[1], x[2])) \\\n",
    "      .map(lambda x : dict(zip(['CategoryID', 'Name', 'Description'], x)))\n",
    "cat2.take(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filter method takes a lambda that returns a True or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Beverages', 'Soft drinks coffees teas beers and ales'),\n",
       " (2, 'Condiments', 'Sweet and savory sauces relishes spreads and seasonings'),\n",
       " (3, 'Confections', 'Desserts candies and sweet breads'),\n",
       " (4, 'Dairy Products', 'Cheeses'),\n",
       " (5, 'Grains/Cereals', 'Breads crackers pasta and cereal')]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat1.filter(lambda x : x[0] <= 5).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filter expressions can be more complicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'CategoryID': 2,\n",
       "  'Name': 'Condiments',\n",
       "  'Description': 'Sweet and savory sauces relishes spreads and seasonings'},\n",
       " {'CategoryID': 6, 'Name': 'Meat/Poultry', 'Description': 'Prepared meats'},\n",
       " {'CategoryID': 8, 'Name': 'Seafood', 'Description': 'Seaweed and fish'}]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat2.filter(lambda x : x['CategoryID'] % 2 == 0 and 'e' in x['Name']).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sortBy method returns an expression that is used to sort the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 'Grains/Cereals', 'Breads crackers pasta and cereal'),\n",
       " (4, 'Dairy Products', 'Cheeses'),\n",
       " (3, 'Confections', 'Desserts candies and sweet breads'),\n",
       " (7, 'Produce', 'Dried fruit and bean curd'),\n",
       " (6, 'Meat/Poultry', 'Prepared meats'),\n",
       " (8, 'Seafood', 'Seaweed and fish'),\n",
       " (1, 'Beverages', 'Soft drinks coffees teas beers and ales'),\n",
       " (2, 'Condiments', 'Sweet and savory sauces relishes spreads and seasonings')]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat1.sortBy(lambda x : x[2]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sortBy has an option ascending parameter to sort in reverse order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 'Seafood', 'Seaweed and fish'),\n",
       " (7, 'Produce', 'Dried fruit and bean curd'),\n",
       " (6, 'Meat/Poultry', 'Prepared meats'),\n",
       " (5, 'Grains/Cereals', 'Breads crackers pasta and cereal'),\n",
       " (4, 'Dairy Products', 'Cheeses'),\n",
       " (3, 'Confections', 'Desserts candies and sweet breads'),\n",
       " (2, 'Condiments', 'Sweet and savory sauces relishes spreads and seasonings'),\n",
       " (1, 'Beverages', 'Soft drinks coffees teas beers and ales')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat1.sortBy(lambda x : x[0], ascending = False).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAB: Try to sort region by name and descending order by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape categories from a tuple of 3 elements like (1, 'Beverages', 'Soft drinks') to a tuple with two elements (key, value) like (1, ('Beverages', 'Soft drinks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, ('Beverages', 'Soft drinks coffees teas beers and ales')),\n",
       " (2,\n",
       "  ('Condiments', 'Sweet and savory sauces relishes spreads and seasonings')),\n",
       " (3, ('Confections', 'Desserts candies and sweet breads')),\n",
       " (4, ('Dairy Products', 'Cheeses')),\n",
       " (5, ('Grains/Cereals', 'Breads crackers pasta and cereal')),\n",
       " (6, ('Meat/Poultry', 'Prepared meats')),\n",
       " (7, ('Produce', 'Dried fruit and bean curd')),\n",
       " (8, ('Seafood', 'Seaweed and fish'))]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat3 = cat1.map(lambda x : (x[0], (x[1], x[2])))\n",
    "cat3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sortByKey method does not require a function as a parameter if the data is structured into a tuple of the shape (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, ('Seafood', 'Seaweed and fish')),\n",
       " (7, ('Produce', 'Dried fruit and bean curd')),\n",
       " (6, ('Meat/Poultry', 'Prepared meats')),\n",
       " (5, ('Grains/Cereals', 'Breads crackers pasta and cereal')),\n",
       " (4, ('Dairy Products', 'Cheeses')),\n",
       " (3, ('Confections', 'Desserts candies and sweet breads')),\n",
       " (2,\n",
       "  ('Condiments', 'Sweet and savory sauces relishes spreads and seasonings')),\n",
       " (1, ('Beverages', 'Soft drinks coffees teas beers and ales'))]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat3.sortByKey(ascending=False).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in another CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1,Chai,8,1,10 boxes x 30 bags,18.0,39,0,10,1',\n",
       " '2,Chang,1,1,24 - 12 oz bottles,19.0,17,40,25,1',\n",
       " '3,Aniseed Syrup,1,2,12 - 550 ml bottles,10.0,13,70,25,0',\n",
       " \"4,Chef Anton's Cajun Seasoning,2,2,48 - 6 oz jars,22.0,53,0,0,0\"]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod = shake = sc.textFile('/home/student/ROI/SparkProgram/datasets/northwind/CSV/products')\n",
    "print(prod.count())\n",
    "prod.take(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split it up and just keep the ProductID, ProductName, CategoryID, Price, Quantity values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Chai', 1, 18.0, 39),\n",
       " (2, 'Chang', 1, 19.0, 17),\n",
       " (3, 'Aniseed Syrup', 2, 10.0, 13),\n",
       " (4, \"Chef Anton's Cajun Seasoning\", 2, 22.0, 53),\n",
       " (5, \"Chef Anton's Gumbo Mix\", 2, 21.35, 0)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod1 = prod.map(lambda x : x.split(',')).map(lambda x : (int(x[0]), x[1], int(x[3]), float(x[5]), int(x[6])))\n",
    "prod1.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape it to a key value tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (1, 'Chai', 18.0, 39)),\n",
       " (1, (2, 'Chang', 19.0, 17)),\n",
       " (2, (3, 'Aniseed Syrup', 10.0, 13)),\n",
       " (2, (4, \"Chef Anton's Cajun Seasoning\", 22.0, 53)),\n",
       " (2, (5, \"Chef Anton's Gumbo Mix\", 21.35, 0))]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod2 = prod1.map(lambda x : (x[2], (x[0], x[1], x[3], x[4])))\n",
    "prod2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, ('Beverages', 'Soft drinks coffees teas beers and ales')),\n",
       " (2,\n",
       "  ('Condiments', 'Sweet and savory sauces relishes spreads and seasonings')),\n",
       " (3, ('Confections', 'Desserts candies and sweet breads')),\n",
       " (4, ('Dairy Products', 'Cheeses')),\n",
       " (5, ('Grains/Cereals', 'Breads crackers pasta and cereal')),\n",
       " (6, ('Meat/Poultry', 'Prepared meats')),\n",
       " (7, ('Produce', 'Dried fruit and bean curd')),\n",
       " (8, ('Seafood', 'Seaweed and fish'))]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both c3 and prod2 are in key value tuple format so they can be joined to produce a new tuple of (key, (cat, prod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  (('Beverages', 'Soft drinks coffees teas beers and ales'),\n",
       "   (1, 'Chai', 18.0, 39))),\n",
       " (1,\n",
       "  (('Beverages', 'Soft drinks coffees teas beers and ales'),\n",
       "   (2, 'Chang', 19.0, 17))),\n",
       " (1,\n",
       "  (('Beverages', 'Soft drinks coffees teas beers and ales'),\n",
       "   (24, 'Guarana Fantastica', 4.5, 20))),\n",
       " (1,\n",
       "  (('Beverages', 'Soft drinks coffees teas beers and ales'),\n",
       "   (34, 'Sasquatch Ale', 14.0, 111))),\n",
       " (1,\n",
       "  (('Beverages', 'Soft drinks coffees teas beers and ales'),\n",
       "   (35, 'Steeleye Stout', 18.0, 20))),\n",
       " (1,\n",
       "  (('Beverages', 'Soft drinks coffees teas beers and ales'),\n",
       "   (38, 'Cote de Blaye', 263.5, 17))),\n",
       " (1,\n",
       "  (('Beverages', 'Soft drinks coffees teas beers and ales'),\n",
       "   (39, 'Chartreuse verte', 18.0, 69))),\n",
       " (1,\n",
       "  (('Beverages', 'Soft drinks coffees teas beers and ales'),\n",
       "   (43, 'Ipoh Coffee', 46.0, 17))),\n",
       " (1,\n",
       "  (('Beverages', 'Soft drinks coffees teas beers and ales'),\n",
       "   (67, 'Laughing Lumberjack Lager', 14.0, 52))),\n",
       " (1,\n",
       "  (('Beverages', 'Soft drinks coffees teas beers and ales'),\n",
       "   (70, 'Outback Lager', 15.0, 15))),\n",
       " (1,\n",
       "  (('Beverages', 'Soft drinks coffees teas beers and ales'),\n",
       "   (75, 'Rhonbrau Klosterbier', 7.75, 125))),\n",
       " (1,\n",
       "  (('Beverages', 'Soft drinks coffees teas beers and ales'),\n",
       "   (76, 'Lakkalikoori', 18.0, 57))),\n",
       " (2,\n",
       "  (('Condiments', 'Sweet and savory sauces relishes spreads and seasonings'),\n",
       "   (3, 'Aniseed Syrup', 10.0, 13))),\n",
       " (2,\n",
       "  (('Condiments', 'Sweet and savory sauces relishes spreads and seasonings'),\n",
       "   (4, \"Chef Anton's Cajun Seasoning\", 22.0, 53))),\n",
       " (2,\n",
       "  (('Condiments', 'Sweet and savory sauces relishes spreads and seasonings'),\n",
       "   (5, \"Chef Anton's Gumbo Mix\", 21.35, 0)))]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined = cat3.join(prod2)\n",
    "joined.sortByKey().take(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAB: Load territories into HDFS and join it to regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The groupBy methods are seldom used but they can produce hierarchies where children records are embedded inside a parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Chai', 18.0, 39),\n",
       " (2, 'Chang', 19.0, 17),\n",
       " (24, 'Guarana Fantastica', 4.5, 20),\n",
       " (34, 'Sasquatch Ale', 14.0, 111),\n",
       " (35, 'Steeleye Stout', 18.0, 20),\n",
       " (38, 'Cote de Blaye', 263.5, 17),\n",
       " (39, 'Chartreuse verte', 18.0, 69),\n",
       " (43, 'Ipoh Coffee', 46.0, 17),\n",
       " (67, 'Laughing Lumberjack Lager', 14.0, 52),\n",
       " (70, 'Outback Lager', 15.0, 15),\n",
       " (75, 'Rhonbrau Klosterbier', 7.75, 125),\n",
       " (76, 'Lakkalikoori', 18.0, 57)]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(group1.take(1)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, <pyspark.resultiterable.ResultIterable at 0x7f9c154c4860>),\n",
       " (2, <pyspark.resultiterable.ResultIterable at 0x7f9c154c4278>),\n",
       " (7, <pyspark.resultiterable.ResultIterable at 0x7f9c154c4048>)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group1 = prod2.groupByKey()\n",
    "group1.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: 1\n",
      "(1, 'Chai', 18.0, 39)\n",
      "(2, 'Chang', 19.0, 17)\n",
      "(24, 'Guarana Fantastica', 4.5, 20)\n",
      "(34, 'Sasquatch Ale', 14.0, 111)\n",
      "(35, 'Steeleye Stout', 18.0, 20)\n",
      "(38, 'Cote de Blaye', 263.5, 17)\n",
      "(39, 'Chartreuse verte', 18.0, 69)\n",
      "(43, 'Ipoh Coffee', 46.0, 17)\n",
      "(67, 'Laughing Lumberjack Lager', 14.0, 52)\n",
      "(70, 'Outback Lager', 15.0, 15)\n",
      "(75, 'Rhonbrau Klosterbier', 7.75, 125)\n",
      "(76, 'Lakkalikoori', 18.0, 57)\n",
      "Key: 2\n",
      "(3, 'Aniseed Syrup', 10.0, 13)\n",
      "(4, \"Chef Anton's Cajun Seasoning\", 22.0, 53)\n",
      "(5, \"Chef Anton's Gumbo Mix\", 21.35, 0)\n",
      "(6, \"Grandma's Boysenberry Spread\", 25.0, 120)\n",
      "(8, 'Northwoods Cranberry Sauce', 40.0, 6)\n",
      "(15, 'Genen Shouyu', 13.0, 39)\n",
      "(44, 'Gula Malacca', 19.45, 27)\n",
      "(61, \"Sirop d'erable\", 28.5, 113)\n",
      "(63, 'Vegie-spread', 43.9, 24)\n",
      "(65, 'Louisiana Fiery Hot Pepper Sauce', 21.05, 76)\n",
      "(66, 'Louisiana Hot Spiced Okra', 17.0, 4)\n",
      "(77, 'Original Frankfurter grune Sosse', 13.0, 32)\n",
      "Key: 7\n",
      "(7, \"Uncle Bob's Organic Dried Pears\", 30.0, 15)\n",
      "(14, 'Tofu', 23.25, 35)\n",
      "(28, 'Rossle Sauerkraut', 45.6, 26)\n",
      "(51, 'Manjimup Dried Apples', 53.0, 20)\n",
      "(74, 'Longlife Tofu', 10.0, 4)\n",
      "Key: 6\n",
      "(9, 'Mishi Kobe Niku', 97.0, 29)\n",
      "(17, 'Alice Mutton', 39.0, 0)\n",
      "(29, 'Thuringer Rostbratwurst', 123.79, 0)\n",
      "(53, 'Perth Pasties', 32.8, 0)\n",
      "(54, 'Tourtiere', 7.45, 21)\n",
      "(55, 'Pate chinois', 24.0, 115)\n",
      "Key: 8\n",
      "(10, 'Ikura', 31.0, 31)\n",
      "(13, 'Konbu', 6.0, 24)\n",
      "(18, 'Carnarvon Tigers', 62.5, 42)\n",
      "(30, 'Nord-Ost Matjeshering', 25.89, 10)\n",
      "(36, 'Inlagd Sill', 19.0, 112)\n",
      "(37, 'Gravad lax', 26.0, 11)\n",
      "(40, 'Boston Crab Meat', 18.4, 123)\n",
      "(41, \"Jack's New England Clam Chowder\", 9.65, 85)\n",
      "(45, 'Rogede sild', 9.5, 5)\n",
      "(46, 'Spegesild', 12.0, 95)\n",
      "(58, 'Escargots de Bourgogne', 13.25, 62)\n",
      "(73, 'Rod Kaviar', 15.0, 101)\n",
      "Key: 4\n",
      "(11, 'Queso Cabrales', 21.0, 22)\n",
      "(12, 'Queso Manchego La Pastora', 38.0, 86)\n",
      "(31, 'Gorgonzola Telino', 12.5, 0)\n",
      "(32, 'Mascarpone Fabioli', 32.0, 9)\n",
      "(33, 'Geitost', 2.5, 112)\n",
      "(59, 'Raclette Courdavault', 55.0, 79)\n",
      "(60, 'Camembert Pierrot', 34.0, 19)\n",
      "(69, 'Gudbrandsdalsost', 36.0, 26)\n",
      "(71, 'Flotemysost', 21.5, 26)\n",
      "(72, 'Mozzarella di Giovanni', 34.8, 14)\n",
      "Key: 3\n",
      "(16, 'Pavlova', 17.45, 29)\n",
      "(19, 'Teatime Chocolate Biscuits', 9.2, 25)\n",
      "(20, \"Sir Rodney's Marmalade\", 81.0, 40)\n",
      "(21, \"Sir Rodney's Scones\", 10.0, 3)\n",
      "(25, 'NuNuCa Nuss-Nougat-Creme', 14.0, 76)\n",
      "(26, 'Gumbar Gummibarchen', 31.23, 15)\n",
      "(27, 'Schoggi Schokolade', 43.9, 49)\n",
      "(47, 'Zaanse koeken', 9.5, 36)\n",
      "(48, 'Chocolade', 12.75, 15)\n",
      "(49, 'Maxilaku', 20.0, 10)\n",
      "(50, 'Valkoinen suklaa', 16.25, 65)\n",
      "(62, 'Tarte au sucre', 49.3, 17)\n",
      "(68, 'Scottish Longbreads', 12.5, 6)\n",
      "Key: 5\n",
      "(22, \"Gustaf's Knackebrod\", 21.0, 104)\n",
      "(23, 'Tunnbrod', 9.0, 61)\n",
      "(42, 'Singaporean Hokkien Fried Mee', 14.0, 26)\n",
      "(52, 'Filo Mix', 7.0, 38)\n",
      "(56, 'Gnocchi di nonna Alice', 38.0, 21)\n",
      "(57, 'Ravioli Angelo', 19.5, 36)\n",
      "(64, 'Wimmers gute Semmelknodel', 33.25, 22)\n"
     ]
    }
   ],
   "source": [
    "group2 = [(key, list(it)) for key, it in group1.collect()]\n",
    "for k,v in group2:\n",
    "    print ('Key:', k)\n",
    "    for x in v:\n",
    "        print(x)\n",
    "#print (group2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduce methods take a function as a parameter that tells spark how to accumulate the values for each group. The function takes two parameters, the first is the accumulated value and the second is the next value in the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 506672),\n",
       " ('the', 23407),\n",
       " ('I', 19540),\n",
       " ('and', 18358),\n",
       " ('to', 15682),\n",
       " ('of', 15649),\n",
       " ('a', 12586),\n",
       " ('my', 10824),\n",
       " ('in', 9633),\n",
       " ('you', 9129)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shake4.map(lambda x : (x, 1)).reduceByKey(lambda x, y : x + y).sortBy(lambda x : x[1], ascending = False).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAB: Use the territories RDD to count how many territories are in each region. \n",
    "Display the results in regionID order and then descending order based on the counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we are adding up all the prices for each categoryID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 455.75),\n",
       " (2, 274.25),\n",
       " (7, 161.85),\n",
       " (6, 324.04),\n",
       " (8, 248.19),\n",
       " (4, 287.3),\n",
       " (3, 327.08),\n",
       " (5, 141.75)]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red1 = prod2.map(lambda x : (x[0], x[1][2])).reduceByKey(lambda x, y: x + y)\n",
    "red1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To accumulate more than one value, use a tuple to hold as many values as you want to aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (455.75, 559, 12)),\n",
       " (2, (274.25, 507, 12)),\n",
       " (7, (161.85, 100, 5)),\n",
       " (6, (324.04, 165, 6)),\n",
       " (8, (248.19, 701, 12)),\n",
       " (4, (287.3, 393, 10)),\n",
       " (3, (327.08, 386, 13)),\n",
       " (5, (141.75, 308, 7))]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red1 = prod2.map(lambda x : (x[0], (x[1][2], x[1][3], 1))).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1], x[2] + y[2]))\n",
    "red1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some python magic can make things easier in the long run.\n",
    "Named tuples make accessing the elements of the row easier\n",
    "Unpacking using the * is a neat python trick that is widely used\n",
    "datetime has function to convert a string into a date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mort = sc.textFile('/home/student/ROI/SparkProgram/datasets/finance/30YearMortgage.csv')\n",
    "head = mort.first()\n",
    "mort = mort.filter(lambda x : x != head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rate(date=datetime.date(1971, 4, 1), fed_fund_rate=0.0415, avg_rate_30year=0.0731),\n",
       " Rate(date=datetime.date(1971, 5, 1), fed_fund_rate=0.0463, avg_rate_30year=0.07425),\n",
       " Rate(date=datetime.date(1972, 2, 1), fed_fund_rate=0.0329, avg_rate_30year=0.07325),\n",
       " Rate(date=datetime.date(1979, 8, 1), fed_fund_rate=0.1094, avg_rate_30year=0.11094),\n",
       " Rate(date=datetime.date(1979, 9, 1), fed_fund_rate=0.1143, avg_rate_30year=0.113)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import date, datetime\n",
    "from collections import namedtuple\n",
    "Rate = namedtuple('Rate','date fed_fund_rate avg_rate_30year')\n",
    "mort1 = mort.map(lambda x : Rate(*(x.split(','))))\n",
    "mort2 = mort1.map(lambda x : Rate(datetime.strptime(x.date, '%Y-%m').date(), float(x.fed_fund_rate), float(x.avg_rate_30year)))\n",
    "mort2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rate(date=datetime.date(1979, 8, 1), fed_fund_rate=0.1094, avg_rate_30year=0.11094),\n",
       " Rate(date=datetime.date(1979, 9, 1), fed_fund_rate=0.1143, avg_rate_30year=0.113),\n",
       " Rate(date=datetime.date(1979, 10, 1), fed_fund_rate=0.1377, avg_rate_30year=0.11637499999999999),\n",
       " Rate(date=datetime.date(1979, 11, 1), fed_fund_rate=0.1318, avg_rate_30year=0.1283),\n",
       " Rate(date=datetime.date(1979, 12, 1), fed_fund_rate=0.1378, avg_rate_30year=0.129),\n",
       " Rate(date=datetime.date(1980, 1, 1), fed_fund_rate=0.1382, avg_rate_30year=0.128775),\n",
       " Rate(date=datetime.date(1980, 2, 1), fed_fund_rate=0.1413, avg_rate_30year=0.1304),\n",
       " Rate(date=datetime.date(1980, 3, 1), fed_fund_rate=0.17190000000000003, avg_rate_30year=0.15282500000000002),\n",
       " Rate(date=datetime.date(1980, 4, 1), fed_fund_rate=0.1761, avg_rate_30year=0.16325),\n",
       " Rate(date=datetime.date(1980, 5, 1), fed_fund_rate=0.10980000000000001, avg_rate_30year=0.14262),\n",
       " Rate(date=datetime.date(1980, 9, 1), fed_fund_rate=0.10869999999999999, avg_rate_30year=0.131975),\n",
       " Rate(date=datetime.date(1980, 10, 1), fed_fund_rate=0.1281, avg_rate_30year=0.13792),\n",
       " Rate(date=datetime.date(1980, 11, 1), fed_fund_rate=0.1585, avg_rate_30year=0.14205),\n",
       " Rate(date=datetime.date(1980, 12, 1), fed_fund_rate=0.18899999999999997, avg_rate_30year=0.1479),\n",
       " Rate(date=datetime.date(1981, 1, 1), fed_fund_rate=0.19079999999999997, avg_rate_30year=0.14904),\n",
       " Rate(date=datetime.date(1981, 2, 1), fed_fund_rate=0.1593, avg_rate_30year=0.15132500000000002),\n",
       " Rate(date=datetime.date(1981, 3, 1), fed_fund_rate=0.147, avg_rate_30year=0.154),\n",
       " Rate(date=datetime.date(1981, 4, 1), fed_fund_rate=0.1572, avg_rate_30year=0.1558),\n",
       " Rate(date=datetime.date(1981, 5, 1), fed_fund_rate=0.1852, avg_rate_30year=0.16402),\n",
       " Rate(date=datetime.date(1981, 6, 1), fed_fund_rate=0.191, avg_rate_30year=0.16695000000000002),\n",
       " Rate(date=datetime.date(1981, 7, 1), fed_fund_rate=0.19039999999999999, avg_rate_30year=0.16832),\n",
       " Rate(date=datetime.date(1981, 8, 1), fed_fund_rate=0.1782, avg_rate_30year=0.17285),\n",
       " Rate(date=datetime.date(1981, 9, 1), fed_fund_rate=0.15869999999999998, avg_rate_30year=0.1816),\n",
       " Rate(date=datetime.date(1981, 10, 1), fed_fund_rate=0.1508, avg_rate_30year=0.18454),\n",
       " Rate(date=datetime.date(1981, 11, 1), fed_fund_rate=0.1331, avg_rate_30year=0.17825),\n",
       " Rate(date=datetime.date(1981, 12, 1), fed_fund_rate=0.12369999999999999, avg_rate_30year=0.16946000000000003),\n",
       " Rate(date=datetime.date(1982, 1, 1), fed_fund_rate=0.1322, avg_rate_30year=0.17485),\n",
       " Rate(date=datetime.date(1982, 2, 1), fed_fund_rate=0.1478, avg_rate_30year=0.175975),\n",
       " Rate(date=datetime.date(1982, 3, 1), fed_fund_rate=0.14679999999999999, avg_rate_30year=0.1716),\n",
       " Rate(date=datetime.date(1982, 4, 1), fed_fund_rate=0.1494, avg_rate_30year=0.16892),\n",
       " Rate(date=datetime.date(1982, 5, 1), fed_fund_rate=0.1445, avg_rate_30year=0.16677499999999998),\n",
       " Rate(date=datetime.date(1982, 6, 1), fed_fund_rate=0.14150000000000001, avg_rate_30year=0.166975),\n",
       " Rate(date=datetime.date(1982, 7, 1), fed_fund_rate=0.1259, avg_rate_30year=0.16815999999999998),\n",
       " Rate(date=datetime.date(1982, 8, 1), fed_fund_rate=0.1012, avg_rate_30year=0.16269999999999998),\n",
       " Rate(date=datetime.date(1982, 9, 1), fed_fund_rate=0.10310000000000001, avg_rate_30year=0.1543),\n",
       " Rate(date=datetime.date(1984, 4, 1), fed_fund_rate=0.10289999999999999, avg_rate_30year=0.136525),\n",
       " Rate(date=datetime.date(1984, 5, 1), fed_fund_rate=0.1032, avg_rate_30year=0.13942500000000002),\n",
       " Rate(date=datetime.date(1984, 6, 1), fed_fund_rate=0.1106, avg_rate_30year=0.14416),\n",
       " Rate(date=datetime.date(1984, 7, 1), fed_fund_rate=0.11230000000000001, avg_rate_30year=0.146675),\n",
       " Rate(date=datetime.date(1984, 8, 1), fed_fund_rate=0.1164, avg_rate_30year=0.1447),\n",
       " Rate(date=datetime.date(1984, 9, 1), fed_fund_rate=0.113, avg_rate_30year=0.1435),\n",
       " Rate(date=datetime.date(1973, 7, 1), fed_fund_rate=0.10400000000000001, avg_rate_30year=0.0805),\n",
       " Rate(date=datetime.date(1973, 8, 1), fed_fund_rate=0.105, avg_rate_30year=0.08496000000000001),\n",
       " Rate(date=datetime.date(1973, 9, 1), fed_fund_rate=0.10779999999999999, avg_rate_30year=0.08814999999999999),\n",
       " Rate(date=datetime.date(1973, 10, 1), fed_fund_rate=0.1001, avg_rate_30year=0.0877),\n",
       " Rate(date=datetime.date(1973, 11, 1), fed_fund_rate=0.1003, avg_rate_30year=0.08582000000000001),\n",
       " Rate(date=datetime.date(1974, 4, 1), fed_fund_rate=0.1051, avg_rate_30year=0.085825),\n",
       " Rate(date=datetime.date(1974, 5, 1), fed_fund_rate=0.1131, avg_rate_30year=0.08972),\n",
       " Rate(date=datetime.date(1974, 6, 1), fed_fund_rate=0.1193, avg_rate_30year=0.09085000000000001),\n",
       " Rate(date=datetime.date(1974, 7, 1), fed_fund_rate=0.1292, avg_rate_30year=0.0928),\n",
       " Rate(date=datetime.date(1974, 8, 1), fed_fund_rate=0.1201, avg_rate_30year=0.09586),\n",
       " Rate(date=datetime.date(1974, 9, 1), fed_fund_rate=0.1134, avg_rate_30year=0.099575),\n",
       " Rate(date=datetime.date(1974, 10, 1), fed_fund_rate=0.10060000000000001, avg_rate_30year=0.09977499999999999),\n",
       " Rate(date=datetime.date(1978, 12, 1), fed_fund_rate=0.1003, avg_rate_30year=0.10346),\n",
       " Rate(date=datetime.date(1979, 1, 1), fed_fund_rate=0.1007, avg_rate_30year=0.1039),\n",
       " Rate(date=datetime.date(1979, 2, 1), fed_fund_rate=0.10060000000000001, avg_rate_30year=0.104075),\n",
       " Rate(date=datetime.date(1979, 3, 1), fed_fund_rate=0.1009, avg_rate_30year=0.10426),\n",
       " Rate(date=datetime.date(1979, 4, 1), fed_fund_rate=0.1001, avg_rate_30year=0.104975),\n",
       " Rate(date=datetime.date(1979, 5, 1), fed_fund_rate=0.1024, avg_rate_30year=0.1069),\n",
       " Rate(date=datetime.date(1979, 6, 1), fed_fund_rate=0.10289999999999999, avg_rate_30year=0.11036),\n",
       " Rate(date=datetime.date(1979, 7, 1), fed_fund_rate=0.1047, avg_rate_30year=0.110925)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mort2.filter(lambda x : x.fed_fund_rate > .1 ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
