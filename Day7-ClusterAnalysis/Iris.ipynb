{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "rootpath = '/home/student/ROI/SparkProgram/'\n",
    "datapath = f'{rootpath}datasets/'\n",
    "sys.path.append(rootpath)\n",
    "from pyspark_helpers import *\n",
    "sc, spark, conf = initspark()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "print(iris.keys())\n",
    "print(iris['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_features = iris['data']\n",
    "iris_label = iris['target']\n",
    "features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "\n",
    "data1 = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= features + ['target'])\n",
    "irisDF = spark.createDataFrame(data1)\n",
    "display(irisDF)\n",
    "\n",
    "irisPandas = irisDF.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vecAssembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "dfML = vecAssembler.transform(irisDF)\n",
    "display(dfML)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "CLUSTERS = 3\n",
    "\n",
    "kmeans = KMeans(k=3, seed = 1)\n",
    "model = kmeans.fit(dfML)\n",
    "centroids = model.clusterCenters()\n",
    "#print(centroids)\n",
    "\n",
    "predictions = model.transform(dfML) \n",
    "#display(predictions)\n",
    "\n",
    "\n",
    "x = predictions.select('prediction', 'target').collect()\n",
    "print(x[0])\n",
    "print('-->', tuple(x[0]))\n",
    "print(x[0].prediction, x[0]['target'])\n",
    "\n",
    "# good recipe to convert list of Row objects into list of tuples\n",
    "print(list(map(tuple, x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "display(irisDF)\n",
    "sql = \"SELECT *, sepal_length/sepal_width as sepal_ratio, petal_length/petal_width as petal_ratio FROM __THIS__\"\n",
    "sqlTransformer = SQLTransformer(statement = sql)\n",
    "sqlModel = sqlTransformer.transform(irisDF)\n",
    "display(sqlModel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "display(irisDF.describe())\n",
    "\n",
    "standardScaler = StandardScaler(inputCol='features', outputCol='scaled_features')\n",
    "model = standardScaler.fit(dfML)\n",
    "print(model.mean, model.std)\n",
    "dfMLScaled = model.transform(dfML)\n",
    "display(dfML)\n",
    "display(dfMLScaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "minMaxScaler = MinMaxScaler(inputCol='features', outputCol='minmax_features')\n",
    "display(minMaxScaler.fit(dfML).transform(dfML))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.ml.linalg import Vectors\n",
    "#from pyspark.ml.feature import QuantileDiscretizer\n",
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "Rform = RFormula(formula = 'target~sepal_length + sepal_width + petal_length + petal_width').fit(irisDF).transform(irisDF)\n",
    "display(Rform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "quantile = QuantileDiscretizer(inputCol='sepal_width', outputCol='sepal_width_bucket', numBuckets = 3).fit(irisDF).transform(irisDF)\n",
    "\n",
    "display(quantile, 50)\n",
    "\n",
    "# d = {'inputCol':'sepal_length', 'outputCol'='sepal_length_bucket', 'numBuckets' = 2}\n",
    "# d1 = dict(inputCol='sepal_length', outputCol='sepal_length_bucket', numBuckets = 2)\n",
    "# quantile1 = QuantileDiscretizer(**d)\n",
    "\n",
    "quantile1 = QuantileDiscretizer(inputCol='sepal_length', outputCol='sepal_length_bucket', numBuckets = 2)\n",
    "quantile2 = QuantileDiscretizer(inputCol='sepal_width', outputCol='sepal_width_bucket', numBuckets = 2)\n",
    "quantile3 = QuantileDiscretizer(inputCol='petal_length', outputCol='petal_length_bucket', numBuckets = 2)\n",
    "quantile4 = QuantileDiscretizer(inputCol='petal_width', outputCol='petal_width_bucket', numBuckets = 2)\n",
    "\n",
    "\n",
    "sql = \"SELECT *, sepal_length * sepal_length_bucket as sl FROM __THIS__\"\n",
    "sqlTransformer = SQLTransformer(statement = sql)\n",
    "\n",
    "\n",
    "Rform = RFormula(formula = 'target~sepal_length_bucket + sepal_width_bucket + petal_length_bucket + petal_width_bucket+sl')\n",
    "\n",
    "stages = [quantile1, quantile2, sqlTransformer, quantile3, quantile4, Rform]\n",
    "\n",
    "pipeline = Pipeline(stages = stages)\n",
    "\n",
    "dfPipe = pipeline.fit(irisDF).transform(irisDF)\n",
    "display(dfPipe)\n",
    "# dfPipe2 = dfPipe.select('label', 'features')\n",
    "# display(dfPipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "#    , 'SQL' : (SQLTransformer, {'statement':sql})\n",
    "#    , 'RFormula' : (RFormula, {'formula':'target~sepal_length + sepal_width + petal_length + petal_width'})\n",
    "#    , 'quantile' : (QuantileDiscretizer, {'inputOutput':True, 'numBuckets':2})\n",
    "\n",
    "\n",
    "scalers = {\n",
    "      'standard' : (StandardScaler,{})\n",
    "    , 'maxAbs' : (MaxAbsScaler,{})\n",
    "    , 'minMax' : (MinMaxScaler,{})\n",
    "    , 'PCA' : (PCA, {'k':2})\n",
    "}\n",
    "\n",
    "for name, scalerClass in scalers.items():\n",
    "    print (name)\n",
    "    c, p = scalerClass\n",
    "    p.update({'inputCol':'features', 'outputCol':name+'_features'})\n",
    "    print (p)\n",
    "    scaler = c(**p)\n",
    "\n",
    "    model = scaler.fit(dfML).transform(dfML)\n",
    "    display(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
